from huggingface_hub import InferenceClient

def ask_hugging_face(prompt: str, hf_token: str, model: str = "mistralai/Mistral-7B-Instruct-v0.3"):
    """
    Sends a prompt to a Hugging Face model and returns the response.

    Args:
        prompt: The input text to send to the model.
        hf_token: Your Hugging Face API token.
        model: The model to use for the inference.

    Returns:
        The text generated by the model.

    Raises:
        RuntimeError: If the API token is not provided or if there's an API error.
    """
    if not hf_token:
        raise RuntimeError("HF_API_TOKEN not found, please provide a valid token.")

    # Initialize client with the model
    client = InferenceClient(model=model, token=hf_token)

    # Generate text
    try:
        response = client.chat_completion(
            messages=[
                {"role": "user", "content": prompt}
            ],
            # max_tokens=500  # Optional: Limit the length of the response
        )
        
        if response.choices and len(response.choices) > 0:
            generated_text = response.choices[0].message.content
            return generated_text
        else:
            raise RuntimeError("Received an empty response from the API.")
            
    except Exception as e:
        raise RuntimeError(f"Hugging Face API Error: {e}")